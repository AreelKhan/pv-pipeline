{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from os import path, makedirs\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "aws_secrets = json.load(open(\"secrets.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket and paths\n",
    "BUCKET_NAME = \"oedi-data-lake\"\n",
    "\n",
    "PARENT_PREFIX = \"pvdaq/parquet/\"\n",
    "system_PREFIX = PARENT_PREFIX + \"site/\"\n",
    "MOUNT_PREFIX =  PARENT_PREFIX + \"mount/\"\n",
    "\n",
    "METRICS_PREFIX = PARENT_PREFIX + \"metrics/metrics__system_{ss_id}\"\n",
    "PV_PREFIX = PARENT_PREFIX + \"pvdata/system_id={ss_id}/year={year}/month={month}/day={day}/\"\n",
    "\n",
    "# Local location for temporary data storage\n",
    "STAGING_AREA = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVExtract:\n",
    "\n",
    "    def __init__(self, aws_access_key_id: str, aws_secret_access_key: str, region_name: str):\n",
    "        self.aws_access_key_id = aws_access_key_id\n",
    "        self.aws_secret_access_key = aws_secret_access_key\n",
    "        self.region_name = region_name\n",
    "\n",
    "        self.s3 = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "\n",
    "    def s3_download(self, key: str, filename: str):\n",
    "        \"\"\"\n",
    "            Given an AWS S3 file key, download and store in the staging destination.\n",
    "            File is named filename\n",
    "        \"\"\"\n",
    "        self.s3.download_file(BUCKET_NAME, key, filename)\n",
    "\n",
    "    def extract_metadata(self) -> None:\n",
    "        \"\"\"\n",
    "            Extracts PV system metadata\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def extract_metrics(self, ss_id: int) -> None:\n",
    "        \"\"\"\n",
    "            Extracts metrics given an ss_id\n",
    "        \"\"\"\n",
    "        metrics_aws_path = METRICS_PREFIX.replace(\"{ss_id}\", str(ss_id))\n",
    "        metrics_object = self.s3.list_objects(Bucket=BUCKET_NAME, Prefix=metrics_aws_path, Delimiter=\"/\")\n",
    "        try:\n",
    "            metrics_key = metrics_object[\"Contents\"][0][\"Key\"]\n",
    "            local_dir = path.join(STAGING_AREA, f\"system_{ss_id}\")\n",
    "            makedirs(local_dir, exist_ok=True)\n",
    "            self.s3_download(metrics_key, path.join(local_dir, f\"metrics_system{ss_id}.parquet\"))\n",
    "        except:\n",
    "            logging.error(f\"Metrics for system: {ss_id} were not found. Aborting extraction.\")\n",
    "        \n",
    "    def extract_pv_data(self, ss_id: int, date: datetime) -> None:\n",
    "        \"\"\"\n",
    "            Extracts pv data given an ss_id and date\n",
    "        \"\"\"\n",
    "        pv_aws_path = PV_PREFIX.replace(\"{ss_id}\", str(ss_id)).replace(\"{year}\", str(date.year)).replace(\"{month}\", str(date.month)).replace(\"{day}\", str(date.day))\n",
    "        pv_object = self.s3.list_objects(Bucket=BUCKET_NAME, Prefix=pv_aws_path, Delimiter=\"/\")\n",
    "        try:\n",
    "            pv_data_key = pv_object[\"Contents\"][0][\"Key\"]\n",
    "            local_dir = path.join(STAGING_AREA, f\"system_{ss_id}\", \"pv_data\")\n",
    "            makedirs(local_dir, exist_ok=True)\n",
    "            self.s3_download(pv_data_key, path.join(local_dir, f\"pv_data_system{ss_id}_{date.strftime(\"%Y-%m-%d\")}.parquet\"))\n",
    "        except:\n",
    "            logging.error(f\"PV data for System: {ss_id} on {date} was not found. Aborting extraction.\")\n",
    "\n",
    "    def extract(self, ss_id: int, start_date: datetime, end_date: datetime) -> None:\n",
    "        \"\"\"\n",
    "            Extracts pv data and associated metrics for given ss_id and date\n",
    "        \"\"\"\n",
    "        # create staging area if it does not exist\n",
    "        makedirs(STAGING_AREA, exist_ok=True)\n",
    "\n",
    "        # check if system metadata exists, if not extract\n",
    "        if not path.isfile(path.join(STAGING_AREA, \"metadata.parquet\")):\n",
    "            logging.info(\"Metadata is not available. Extracting from source...\")\n",
    "            self.extract_metadata()\n",
    "\n",
    "        # check if metadata exists, if not extract\n",
    "        if not path.isfile(path.join(STAGING_AREA, f\"system_{ss_id}\", f\"metrics_system{ss_id}.parquet\")):\n",
    "            logging.info(f\"Metrics for System {ss_id} are not available. Extracting from source...\")\n",
    "            self.extract_metrics(ss_id)\n",
    "        \n",
    "        # extract pv\n",
    "        logging.info(f\"Extracting PV data for System {ss_id} for dates: {start_date} to {end_date}\")\n",
    "        for date in pd.date_range(start=start_date, end=end_date):\n",
    "            self.extract_pv_data(ss_id, date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Metadata is not available. Extracting from source...\n",
      "INFO:root:Extracting PV data for System 10 for dates: 2013-03-14 00:00:00 to 2013-04-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "aws_secrets = json.load(open(\"./secrets.json\"))\n",
    "pipeline = PVExtract(\n",
    "    aws_access_key_id=aws_secrets[\"aws_access_key_id\"],\n",
    "    aws_secret_access_key=aws_secrets[\"aws_secret_access_key\"],\n",
    "    region_name=aws_secrets[\"region_name\"]\n",
    ")\n",
    "pipeline.extract(ss_id=10, start_date=datetime(2013, 3, 14), end_date=datetime(2013, 4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pv-etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
