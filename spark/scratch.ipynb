{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from os import path, makedirs, remove\n",
    "from logging import Logger\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from typing import List\n",
    "import logging\n",
    "from shutil import rmtree\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# S3 bucket and paths\n",
    "BUCKET_NAME = \"oedi-data-lake\"\n",
    "\n",
    "PARENT_PREFIX = \"pvdaq/parquet/\"\n",
    "SITE_PREFIX = PARENT_PREFIX + \"site/\"\n",
    "MOUNT_PREFIX =  PARENT_PREFIX + \"mount/\"\n",
    "\n",
    "METRICS_PREFIX = PARENT_PREFIX + \"metrics/metrics__system_{ss_id}\"\n",
    "PV_PREFIX = PARENT_PREFIX + \"pvdata/system_id={ss_id}/year={year}/month={month}/day={day}/\"\n",
    "\n",
    "\n",
    "class BaseExtractor:\n",
    "    def __init__(\n",
    "            self,\n",
    "            aws_access_key_id: str,\n",
    "            aws_secret_access_key: str,\n",
    "            region_name: str,\n",
    "            staging_area: str,\n",
    "            logger: Logger\n",
    "            ):\n",
    "        \"\"\"\n",
    "            Initializes the Extract step of the data pipeline\n",
    "        \"\"\"\n",
    "        self.aws_access_key_id = aws_access_key_id\n",
    "        self.aws_secret_access_key = aws_secret_access_key\n",
    "        self.region_name = region_name\n",
    "\n",
    "        self.s3 = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "            region_name=region_name\n",
    "        )\n",
    "\n",
    "        self.staging_area = staging_area\n",
    "        self.logger = logger\n",
    "\n",
    "\n",
    "    def s3_download(self, key: str, filename: str):\n",
    "        \"\"\"\n",
    "            Given an AWS S3 file key, downloads it.\n",
    "            File is named filename.\n",
    "            Assumes filename has valid file path. (director already exists)\n",
    "        \"\"\"\n",
    "        self.s3.download_file(BUCKET_NAME, key, filename)\n",
    "\n",
    "\n",
    "class BaseLoader:\n",
    "     \n",
    "    def __init__(\n",
    "            self,\n",
    "            project_id: str,\n",
    "            credentials_path: str,\n",
    "            staging_area: str,\n",
    "            logger: Logger,\n",
    "            ):\n",
    "        self.project_id = project_id\n",
    "        self.credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "        self.client = bigquery.Client(project=project_id, credentials=self.credentials)\n",
    "        self.staging_area = staging_area\n",
    "        self.logger = logger\n",
    "\n",
    "    def load_to_bq(\n",
    "            self,\n",
    "            dataset_id: str,\n",
    "            table_id: str,\n",
    "            table_schema: List[bigquery.SchemaField],\n",
    "            source_data_path: str,\n",
    "            ):\n",
    "        table_ref = self.client.dataset(dataset_id).table(table_id)\n",
    "        table = bigquery.Table(table_ref, schema=table_schema)\n",
    "\n",
    "        try:\n",
    "            self.client.get_table(table)\n",
    "        except Exception:\n",
    "            self.logger.info(f\"Table {table} is not found. Creating...\")\n",
    "            self.client.create_table(table)\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig()\n",
    "        job_config.source_format = bigquery.SourceFormat.PARQUET\n",
    "        job_config.write_disposition = \"WRITE_APPEND\"\n",
    "        job_config.schema_update_options = ['ALLOW_FIELD_ADDITION', 'ALLOW_FIELD_RELAXATION']\n",
    "\n",
    "        with open(source_data_path, \"rb\") as source_file:\n",
    "            job = self.client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "        job.result()\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "class PVExtract(BaseExtractor):\n",
    "\n",
    "    def extract_metrics(self, ss_id: int) -> None:\n",
    "        \"\"\"\n",
    "            Extracts Metrics given an ss_id\n",
    "        \"\"\"\n",
    "        metrics_aws_path = METRICS_PREFIX.replace(\"{ss_id}\", str(ss_id))\n",
    "        metrics_object = self.s3.list_objects(Bucket=BUCKET_NAME, Prefix=metrics_aws_path, Delimiter=\"/\")\n",
    "        try:\n",
    "            metrics_key = metrics_object[\"Contents\"][0][\"Key\"]\n",
    "            local_dir = path.join(self.staging_area, f\"system{ss_id}\")\n",
    "            makedirs(local_dir, exist_ok=True)\n",
    "            self.s3_download(metrics_key, path.join(local_dir, f\"metrics_system{ss_id}.parquet\"))\n",
    "        except Exception as error:\n",
    "            self.logger.error(f\"Error while extracting metrics for Site {ss_id}: \\n{error}\")\n",
    "\n",
    "\n",
    "    def extract_pv_data(self, ss_id: int, date: datetime) -> None:\n",
    "        \"\"\"\n",
    "            Extracts PV data given an ss_id and date (single day)\n",
    "        \"\"\"\n",
    "        pv_aws_path = PV_PREFIX.replace(\"{ss_id}\", str(ss_id)).replace(\"{year}\", str(date.year)).replace(\"{month}\", str(date.month)).replace(\"{day}\", str(date.day))\n",
    "        pv_object = self.s3.list_objects(Bucket=BUCKET_NAME, Prefix=pv_aws_path, Delimiter=\"/\")\n",
    "        try:\n",
    "            pv_data_key = pv_object[\"Contents\"][0][\"Key\"]\n",
    "            local_dir = path.join(self.staging_area, f\"system{ss_id}\", \"pv_data\")\n",
    "            makedirs(local_dir, exist_ok=True)\n",
    "            self.s3_download(pv_data_key, path.join(local_dir, f\"pv_data_system{ss_id}_{date.strftime('%Y-%m-%d')}.parquet\"))\n",
    "        except Exception as error:\n",
    "            self.logger.error(f\"Error while extracting PV data for Site {ss_id} on {date}: /n{error}\")\n",
    "\n",
    "\n",
    "    def extract(self, ss_id: int, start_date: datetime, end_date: datetime) -> None:\n",
    "        \"\"\"\n",
    "            Extracts PV data and metrics for a given ss_id and date\n",
    "        \"\"\"\n",
    "        makedirs(self.staging_area, exist_ok=True)\n",
    "\n",
    "        self.logger.info(f\"Extracting Metrics for System {ss_id}...\")\n",
    "        self.extract_metrics(ss_id)\n",
    "        \n",
    "        self.logger.info(f\"Extracting PV data for System {ss_id} for dates: {start_date} to {end_date}\")\n",
    "        for date in pd.date_range(start=start_date, end=end_date):\n",
    "            self.extract_pv_data(ss_id, date)\n",
    "\n",
    "\n",
    "\n",
    "class PVTransform:\n",
    "\n",
    "    def __init__(self, staging_area: str, logger: Logger):\n",
    "        self.staging_area = staging_area\n",
    "        self.logger = logger\n",
    "\n",
    "    def transform(self, ss_id: int):\n",
    "        \"\"\"\n",
    "            Transforms all the data present in the staging area for ss_id\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Transforming PV data for System {ss_id}...\")\n",
    "            \n",
    "            metrics_cols = [\"system_id\", \"metric_id\", \"sensor_name\", \"raw_units\"]\n",
    "            metrics = pd.read_parquet(path.join(self.staging_area, f\"system{ss_id}\", f\"metrics_system{ss_id}.parquet\"), columns=metrics_cols)\n",
    "            metrics = metrics[metrics[\"sensor_name\"].isin([\"dc_power\", \"ac_power\", \"poa_irradiance\"])]\n",
    "\n",
    "            pv_cols = [\"measured_on\", \"metric_id\", \"value\"]\n",
    "            pv_data = pd.read_parquet(path.join(self.staging_area, f\"system{ss_id}\", \"pv_data\", f\"pv_data_system{ss_id}_2010-03-09.parquet\"), columns=pv_cols)\n",
    "\n",
    "            renames = {\n",
    "                    \"system_id\":\"ss_id\",\n",
    "                    \"measured_on\":\"timestamp\",\n",
    "                    \"sensor_name\":\"sensor\",\n",
    "                    \"raw_units\":\"units\"\n",
    "                }\n",
    "            merged = pd.merge(pv_data, metrics, \"inner\", \"metric_id\")\n",
    "            merged = merged.rename(renames, axis=1)\n",
    "            merged = merged[[\"ss_id\", \"units\", \"timestamp\", \"value\", \"sensor\"]]\n",
    "            merged = merged.replace(\"\", pd.NA)\n",
    "\n",
    "            for col in merged.columns:\n",
    "                if col not in [\"sensor\", \"units\", \"timestamp\"]:\n",
    "                    merged[col] = pd.to_numeric(merged[col], \"coerce\")\n",
    "\n",
    "            merged.to_parquet(path.join(self.staging_area, \"system10\", \"pv_data_merged.parquet\"))\n",
    "            rmtree(path.join(self.staging_area, f\"system{ss_id}\", \"pv_data/\"))\n",
    "            remove(path.join(self.staging_area, f\"system{ss_id}\", f\"metrics_system{ss_id}.parquet\"))\n",
    "\n",
    "        except Exception as error:\n",
    "            self.logger.error(f\"Error while transforming metadata: /n{error}\")\n",
    "\n",
    "        return None\n",
    "    \n",
    "\n",
    "class PVLoad(BaseLoader):\n",
    "\n",
    "    def load(self, ss_id: int):\n",
    "        table_schema = [\n",
    "            bigquery.SchemaField(\"ss_id\", \"INTEGER\", \"NULLABLE\"),\n",
    "            bigquery.SchemaField(\"timetsamp\", \"DATETIME\"),\n",
    "            bigquery.SchemaField(\"sensor\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"units\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"value\", \"FLOAT\"),\n",
    "            ]\n",
    "        parquet_files = glob.glob(path.join(self.staging_area, f\"system{ss_id}\", \"pv_data_merged.parquet\", \"*.parquet\"))\n",
    "        self.logger.info(f\"Uploading PV data for system {ss_id}\")\n",
    "        for file in parquet_files:\n",
    "            try:\n",
    "                self.load_to_bq(\n",
    "                    dataset_id=\"pv_oedi\",\n",
    "                    table_id=\"pv_data\",\n",
    "                    table_schema=table_schema,\n",
    "                    source_data_path=file\n",
    "                )\n",
    "            except Exception as error:\n",
    "                self.logger.error(f\"Error while loading PV data from {file} into BigQuery: \\n{error}\")\n",
    "                raise error\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PVLoad(\n",
    "    project_id=\"cohere-pv-pipeline\",\n",
    "    credentials_path=\"./bq_service_account_key.json\",\n",
    "    staging_area=\"staging_area\",\n",
    "    logger=logging.getLogger(__name__)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.load(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4320 entries, 0 to 4319\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   ss_id      4320 non-null   int32         \n",
      " 1   timestamp  4320 non-null   datetime64[ns]\n",
      " 2   sensor     4320 non-null   object        \n",
      " 3   units      4320 non-null   object        \n",
      " 4   value      4320 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int32(1), object(2)\n",
      "memory usage: 152.0+ KB\n"
     ]
    }
   ],
   "source": [
    "pd.read_parquet(\"./staging_area/system10/pv_data_merged.parquet/part-00002-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet\").info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = PVExtract(\n",
    "    staging_area=\"staging_area\",\n",
    "    aws_access_key_id=\"AKIA4MTWG33OOIEEML5D\",\n",
    "    aws_secret_access_key=\"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\",\n",
    "    region_name=\"us-west-2\",\n",
    "    logger=logging.getLogger(__name__)\n",
    ")\n",
    "\n",
    "ss_id = 10\n",
    "start_date = datetime.strptime(\"2010/03/01\", \"%Y/%m/%d\")\n",
    "end_date = datetime.strptime(\"2010/03/10\", \"%Y/%m/%d\")\n",
    "\n",
    "extractor.extract(\n",
    "    ss_id=ss_id,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = PVTransform(\n",
    "    staging_area=\"./staging_area/\",\n",
    "    logger=logging.getLogger(__name__)\n",
    ")\n",
    "transformer.transform(ss_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./staging_area/system10/pv_data_merged.parquet\\\\part-00000-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet',\n",
       " './staging_area/system10/pv_data_merged.parquet\\\\part-00001-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet',\n",
       " './staging_area/system10/pv_data_merged.parquet\\\\part-00002-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet',\n",
       " './staging_area/system10/pv_data_merged.parquet\\\\part-00003-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet',\n",
       " './staging_area/system10/pv_data_merged.parquet\\\\part-00004-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet',\n",
       " './staging_area/system10/pv_data_merged.parquet\\\\part-00005-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet',\n",
       " './staging_area/system10/pv_data_merged.parquet\\\\part-00006-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet',\n",
       " './staging_area/system10/pv_data_merged.parquet\\\\part-00007-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet',\n",
       " './staging_area/system10/pv_data_merged.parquet\\\\part-00008-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet',\n",
       " './staging_area/system10/pv_data_merged.parquet\\\\part-00009-d7237e7c-789a-4dc5-a879-cf6686bee8f7-c000.snappy.parquet']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./staging_area/system10/pv_data_merged.parquet/*.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pv-etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
